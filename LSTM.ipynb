{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "def clean_data():\n",
    "    path = 'train.csv'\n",
    "    vector_dimension=300\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    x = data.loc[:,'text'].values\n",
    "    y = data.loc[:,'label'].values\n",
    "\n",
    "    train_size = int(0.8 * len(y))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    xtr = x[:train_size]\n",
    "    xte = x[train_size:]\n",
    "    ytr = y[:train_size]\n",
    "    yte = y[train_size:]\n",
    "\n",
    "    np.save('xtr_shuffled.npy',xtr)\n",
    "    np.save('xte_shuffled.npy',xte)\n",
    "    np.save('ytr_shuffled.npy',ytr)\n",
    "    np.save('yte_shuffled.npy',yte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with KERAS & TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160064    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,365\n",
      "Trainable params: 213,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16051 samples, validate on 4153 samples\n",
      "Epoch 1/5\n",
      "16051/16051 [==============================] - 183s 11ms/step - loss: 0.3043 - acc: 0.8770 - val_loss: 0.1658 - val_acc: 0.9405\n",
      "Epoch 2/5\n",
      "16051/16051 [==============================] - 161s 10ms/step - loss: 0.1370 - acc: 0.9545 - val_loss: 0.2361 - val_acc: 0.9136\n",
      "Epoch 3/5\n",
      "16051/16051 [==============================] - 261s 16ms/step - loss: 0.1119 - acc: 0.9632 - val_loss: 0.2112 - val_acc: 0.9198\n",
      "Epoch 4/5\n",
      "16051/16051 [==============================] - 227s 14ms/step - loss: 0.0939 - acc: 0.9689 - val_loss: 0.2144 - val_acc: 0.9359\n",
      "Epoch 5/5\n",
      "16051/16051 [==============================] - 154s 10ms/step - loss: 0.0921 - acc: 0.9693 - val_loss: 0.2545 - val_acc: 0.9314\n",
      "Accuracy= 93.14%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "\n",
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte, ypred)\n",
    "    plt.show()\n",
    "\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy')\n",
    "xte = np.load('./xte_shuffled.npy')\n",
    "y_train = np.load('./ytr_shuffled.npy')\n",
    "y_test = np.load('./yte_shuffled.npy')\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy= %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Draw the confusion matrix\n",
    "y_pred = model.predict_classes(X_test)\n",
    "plot_cmat(y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
